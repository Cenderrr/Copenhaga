# -*- coding: utf-8 -*-
"""XGBoost Clear Kopenhaga - Wojciech Cendrowski pd4255.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSNy14W2ltWqvhLXkHGgXLNJBG4tXaK2

#1. Import i wstępna analiza danych źródłowych
Jako przedmiot badań zostało wytypowane miasto Copenhaga
"""

#import modułów

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import scipy.stats as ss
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression
from operator import index
from statsmodels.graphics.mosaicplot import mosaic

#import danych z Kopenhagi do zmiennej

data = pd.read_csv("http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2023-12-27/data/listings.csv.gz")

#informacje o danych

data.info()

#RangeIndex: 18545 entries, 0 to 18544 - 18545 obserwacji
#Data columns (total 75 columns): - 75 cech opisujących
#dtypes: float64(21), int64(21), object(33) - 42 wymiarów liczbowych, 33 wymiary nieliczbowe
#memory usage: 10.6+ MB - wielkość pliku

#usunięcie ograniczników w wyświetlaniu danych

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

#nagłówek danych

data.head()

#wyświetlenie 5 losowo wybranych obserwacji

data.sample(5)

"""#2. Podsumowanie zmiennych numerycznych oraz kategorycznych"""

data.describe(include=np.number)

data.describe(exclude=np.number)

"""# 3. Edycja danych

##Stworzenie nowego indexu
"""

#sprawdzenie czy kolumna id zawiera wartości unikalne - można przekonwertować na indeks

data['id'].is_unique

#ustawienie kolumny id jako index data frame

data.set_index('id', inplace=True)

"""##Analiza wartości NaN"""

#w pierwszej kolejności sprawdzam jaki procent wartości NaN zawierają poszczególne kolumny - lista pokazuje jedynie te kolumny, które mają 15% lub więcej nulli

empties = data.isna().sum() / len(data)
empties.sort_values(ascending=False).loc[empties > 0.15] * 100

#kolumna bathroom jest w 100% wypełniona nullami, ale będzie się dało wyciągnąć informację z bathrooms_text, które to z kolei będzie można usunąć już po wykorzystaniu danych
#kolumna bedrooms - 100% nulli, nie da się wyciągnąć danych o liczbie sypialni na podstawie liczby łóżek [beds] ani typu obiektu [room_type], ale liczba zawiera się w nazwie [name], będzie podjęta próba wyciągnięcia tej informacji
#kolumny: license descriptions i callendar update - 100% nulli - do usunięcia
#przyjmuje się, że serie danych pustych w więcej niż 30% odrzuca się ze zbioru, natomiast tutaj zdecydowanie kolumna 'price' z ceną najmu wydaje się jedną z bardziej kluczowych danych, zatem zdecydowano się zachować próg w wysokości 40%
#w związku z powyższym kolumny: host_neighbourhood, host_about, neighborhood_overview, neighbourhood, host_response_time, host_response_rate również ulegną usunięciu

data.drop(columns=['license', 'description', 'calendar_updated', 'host_neighbourhood', 'neighbourhood_group_cleansed', 'host_neighbourhood', 'host_about', 'neighbourhood', 'neighborhood_overview', 'host_response_rate', 'host_response_time'], inplace=True)

#sprawdzenie, czy ramka danych nie zawiera już kolumn usuniętych w poprzednim punkcie

data.info()

"""##Sprawdzenie danych kolumna po kolumnie, konwersja na inne typy danych"""

#definicja funkcji pozwalająca na szybkie rozpoznanie podstawowych informacji o danych w kolumnie

def ColumnReport(checked_column):
  print('badana kolumna:              ', checked_column)
  print('typ danych:                  ', data[checked_column].dtype)
  print('ilość NaN:                   ', (data[checked_column].isna().sum()))
  print('procent NaN:                 ', data[checked_column].isna().sum() / len(data[checked_column]) * 100, '%')
  print('liczba wartości unikalnych:  ', len(data[checked_column].unique()))
  print('procent wartości unikalnych: ', len(data[checked_column].unique()) / len(data[checked_column]) * 100, '%')
  print()
  print('losowo wybrane obserwacje:')
  print()
  print(data[checked_column].sample(3).values)

ColumnReport('scrape_id')

#kolumna scrape id nie wnosi przydatnych informacji (jedna wartość) - do usunięcia

data.drop(columns=['scrape_id'], inplace=True)

ColumnReport('last_scraped')

#konwersja danych w kolumnie last scraped ze string na format datetime

data['last_scraped'] = data['last_scraped'].astype('datetime64[ns]')

ColumnReport('source')

#konwersja danych w kolumnie source ze string na typ kategoryczny (tylko 2 różne wartości), oszczędność miejsca

data['source'] = data['source'].astype('category')

ColumnReport('name')

# kolumna name jest pełna, z różnymi wartościami, string, pozostawiona

ColumnReport('picture_url')

ColumnReport('host_id')

ColumnReport('host_url')

ColumnReport('host_name')

#jedna wartość NaN, sprawdzona zostaje obserwacja pod kątem innych kolumn

data.loc[data['host_name'].isnull()]

#host id jest wypełnione, jednostkowy przypadek, pozostawione

ColumnReport('host_since')

#jedna wartość NaN, sprawdzona zostaje obserwacja pod kątem innych kolumn
#typowy format daty -> konwersja na format datetime

data['host_since'] = data['host_since'].astype('datetime64[ns]')
data.loc[data['host_since'].isnull()]

#ta sama obserwacja, podejrzenie jakiegoś błędu w systemie, użycie jakiegoś niedozwolonego znaku, który nie mógł być zinterpretowany na którymś etapie przetwarzania danych - tylko podejrzenie
#obserwacja ta ma również dużo innych braków w danych dotyczących hosta, natomiast posiada informacje co do samego lokalu, stąd decyzja o jej nieusuwaniu - wartość null -> NaT

ColumnReport('host_location')

ColumnReport('host_acceptance_rate')
data['host_acceptance_rate'].value_counts().head(15)
data['host_acceptance_rate'].values

#rozpoznanie danych wskazuje na możliwą konwersję na wartość liczbową - jako, że jest to % konwersja nastąpi na typ float po usunięciu znaku % i podzieleniu na 100

data['host_acceptance_rate'] = data['host_acceptance_rate'].str.replace(r"%", '', regex=True)
data['host_acceptance_rate'] = data['host_acceptance_rate'].astype(float) / 100

ColumnReport('host_is_superhost')
data['host_is_superhost'].value_counts()

#dane są typu true/false, posiadają jednak 32 wartości NaN - sklasyfikowano je jako False i skonwertowano na typ bool

data['host_is_superhost'] = data['host_is_superhost'].map({'t':True, 'f':False}).fillna(False)
data['host_is_superhost'] = data['host_is_superhost'].astype(bool)

ColumnReport('host_thumbnail_url')
ColumnReport('host_picture_url')

ColumnReport('host_listings_count')

data['host_listings_count'].value_counts()

#data.loc[data['host_listings_count'].isnull()]
#jest to stary znajomy czyli obserwacja z niezidentyfikowanym hostem,

ColumnReport('host_total_listings_count')
data['host_total_listings_count'].value_counts()
data.loc[data['host_total_listings_count'].isnull()]

#sprawdzenie czy host_id występuje gdzieś jeszcze

data.loc[data['host_id'] == 76645464]

#niestety nie występuje nigdzie indziej,

#sprawdzenie zależności pomiędzy host_id a host_listings_count

data['host_id'].astype(str).value_counts().head(10)

#listnings count powinien wskazywać na liczbę ofert, lecz sprawdzając częstość występowania pięciu najczęściej występujących host_id i porównujac ich wartości host_listning_count do liczby obserwacji widać,
#że w 4 na 5 przypadków liczba ta się nie zgadza, zatem nie uznano tutaj zależności

print(data.loc[data['host_id'] == 187610263]['host_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 155348640]['host_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 437864658]['host_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 179753578]['host_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 24943487]['host_listings_count'].value_counts().index)

#dla pewności to samo sprawdzenie ale dla zmiennej host_total_listings_count - tutaj również nie zaobserwowano identyczności

print(data.loc[data['host_id'] == 187610263]['host_total_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 155348640]['host_total_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 437864658]['host_total_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 179753578]['host_total_listings_count'].value_counts().index)
print(data.loc[data['host_id'] == 24943487]['host_total_listings_count'].value_counts().index)

#zdecydowano się podmienić wartość NaN na 0 w obydwu kolumnach, aby móc przetransformować kolumny na typ integer
#sprawdzono, czy 0 występuje w serii danych

print(0 in data['host_listings_count'].values)
print(0 in data['host_total_listings_count'].values)

data['host_listings_count'] = data['host_listings_count'].fillna(0)
data['host_total_listings_count'] = data['host_total_listings_count'].fillna(0)
data['host_listings_count'] = data['host_listings_count'].astype(int)
data['host_total_listings_count'] = data['host_total_listings_count'].astype(int)

ColumnReport('host_verifications')
data['host_verifications'].value_counts()

#rozpoznanie danych wskazuje na listę powtarzalnych elementów

#zamiana na listę

data['host_verifications'].str.replace(r"[[\],']", '', regex=True).str.split()
data['host_verifications'] = data['host_verifications'].apply(eval)

ColumnReport('host_has_profile_pic')
data['host_has_profile_pic'].value_counts()

#rozpoznanie danych -> konwersja na bool ze zmapowaniem NaN na False

data['host_has_profile_pic'] = data['host_has_profile_pic'].map({'t':True, 'f':False}).fillna(False)
data['host_has_profile_pic'] = data['host_has_profile_pic'].astype(bool)

ColumnReport('host_identity_verified')
data['host_identity_verified'].value_counts()

#rozpoznanie danych -> konwersja na bool ze zmapowaniem NaN na False

data['host_identity_verified'] = data['host_identity_verified'].map({'t':True, 'f':False}).fillna(False)
data['host_identity_verified'] = data['host_identity_verified'].astype(bool)

ColumnReport('neighbourhood_cleansed')

ColumnReport('latitude')
ColumnReport('longitude')

ColumnReport('property_type')

#zamiana na typ kategoryczny

data['property_type'] = data['property_type'].astype('category')

ColumnReport('room_type')

#zamiana na typ kategoryczny

data['room_type'] = data['room_type'].astype('category')

data['room_type'].value_counts().plot(kind='bar')

ColumnReport('accommodates')

ColumnReport('bathrooms')
ColumnReport('bathrooms_text')
data['bathrooms_text'].value_counts()

#wyciągnięcie liczby łazienek z kolumny opisowej łazienek i uzupełnienie o te wartości kolumny bathroom, połówki to łazienki częściowe tj np. WC + umywalka, każdy inny przypadek niż half opisane jest liczbą

data['bathrooms'] = np.where(data['bathrooms_text'].str.contains('half', case=False, na=False), 0.5, data['bathrooms_text'].str.split().str[0]).astype('float')

ColumnReport('bedrooms')
ColumnReport('name')

bedrooms_tmp = data['name'].str.split(pat=' bedroom', expand=True)[0]
bedrooms_tmp.str.get(-1).value_counts()

#ze sprawdzenia wynika, że 195 obserwacji przyjęło znak 'h', 17 znak 's' i 1 znak 'd',

#sprawdzenie jak wyglądają pełne nazwy name dla w/w obserwacji z 'h', 's' i 'd'

pd.options.display.max_colwidth = 100
data[['name', 'beds']][(bedrooms_tmp.str.get(-1) == 'h') | (bedrooms_tmp.str.get(-1) == 's') | (bedrooms_tmp.str.get(-1) == 'd')].head(20)

#wynika z tego, że w dla tych obserwacji nie podano w nazwie liczby sypialni bedrooms
#można jednakowoż przyjąć, że apartamenty posiadające 1 łóżko bed będą miały maksymalnie jedną sypialnię bedroom

#sprawdzenie czy występuje w wartościach 0 w kolumnie beds, czy dałoby się zamienić wartości NaN na 0 i skonwertować kolumny bedrooms i beds na int

print(0 in data['beds'].values)

#import danych o liczbie sypialni do kolumny bedrooms
data['bedrooms'] = bedrooms_tmp.str.get(-1)

data['bedrooms'].value_counts()

#zamiana znaków na 0

for ch in ['h', 's', 'd']:
  data['bedrooms'].replace((ch), 0, inplace=True)

# uzupełnienie wartości NaN wartością 1, o ile beds == 1

data['bedrooms'].loc[(data['bedrooms'] == 0) & (data['beds'] == 1)] = 1

data['bedrooms'] = data['bedrooms'].astype(int)

ColumnReport('beds')
data['beds'].value_counts()
data['name'].loc[data['beds'].isna()].head(20)

#niestety, wynika z tego, że nie ma informacji w kolumnie name dotycącą liczby łóżek
#wartości NaN zostaną zamienione na 0 i zamienione na typ int

data['beds'] = data['beds'].fillna(0)
data['beds'] = data['beds'].astype(int)

ColumnReport('amenities')
data['amenities'].value_counts()
data['amenities'].sample(30)

#kolumna amenities zawiera listy 0-elementowe dla 100% obserwacji, kolumna niestety kwalifikuje się do usunięcia

data.drop(columns=['amenities'], inplace=True)

ColumnReport('price')
data['price'].value_counts().head(30)

#typ string łatwy do skonwertowania na wartość liczbową - zdecydowano się użyć typu danych float, z uwagi na możliwe wystąpienia wartości ułamkowych

#wartość jest podana w koronach duńskich DKK, aktualny przelicznik DKK->USD 0.15, usunięcie znaku $ oraz przecinka jako separatora tysięcznego i zamiana na typ float

currency = 0.15

data['price'] = data['price'].str.replace('[$,]', '', regex=True).astype(float)
data['price'] = data['price'] * currency

#wszystkie poniższe kolumny są poprawne pod kątem jakości i wydajności

ColumnReport('maximum_nights')
ColumnReport('minimum_minimum_nights')
ColumnReport('maximum_minimum_nights')
ColumnReport('minimum_maximum_nights')
ColumnReport('maximum_maximum_nights')
ColumnReport('maximum_maximum_nights')
ColumnReport('minimum_nights_avg_ntm')
ColumnReport('maximum_nights_avg_ntm')

ColumnReport('has_availability')
data['has_availability'].value_counts()

#kolumna has_availability posiada powyżej 30% wartości pustych, teoretycznie powinna być w typie boolowskim ale do sprawdzenia potencjalne zależności z kolumnami availability_30 etc.

data[['has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365']].loc[data['has_availability'] == 'f'].sort_values(['availability_365'], ascending=False)
data[['has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365']].loc[data['has_availability'] == 't'].sort_values(['availability_365'], ascending=False)
data[['has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365']].loc[data['has_availability'].isna()].sort_values(['availability_365'], ascending=False)

#z analizy powyższych wynika brak korelacji pomiędzy kolumną has_availability a pozostałymi, kolumna nie dostarczy nam zatem zbyt wielu danych - kolumnę usunięto

data.drop(columns=['has_availability'], inplace=True)

ColumnReport('calendar_last_scraped')

#konwersja na typ datetime

data['calendar_last_scraped'] = data['calendar_last_scraped'].astype('datetime64[ns]')

ColumnReport('number_of_reviews')
ColumnReport('number_of_reviews_ltm')
ColumnReport('number_of_reviews_l30d')
ColumnReport('first_review')
ColumnReport('last_review')

### dodano kolumnę z końca, która również może współgrać z w/w

ColumnReport('reviews_per_month')

print(len(data.loc[data['number_of_reviews'] == 0]))
print(len(data[['number_of_reviews', 'first_review', 'last_review', 'reviews_per_month']].loc[(data['first_review'].isna()) & (data['last_review'].isna()) & (data['reviews_per_month'].isna()) & (data['number_of_reviews'] == 0)]))

#kolumny first_review i last_review mają wartości NaN w sytuacji kiedy wartość == 0 w kolumnie number_of_reviews, innymi słowy nie ma daty recenzji skoro nie ma recenzji, tak samo nie ma recenzji średnio w miesiącu, bo jest 0 - wartość zatem powinna wynosić 0
#wartość NaN zostanie przyjęta jako 1900-01-01

data['first_review'].fillna('1900-01-01', inplace=True)
data['last_review'].fillna('1900-01-01', inplace=True)
data['reviews_per_month'].fillna(0, inplace=True)

data['first_review'] = data['first_review'].astype('datetime64[ns]')
data['last_review'] = data['last_review'].astype('datetime64[ns]')

ColumnReport('review_scores_rating')
ColumnReport('review_scores_accuracy')
ColumnReport('review_scores_cleanliness')
ColumnReport('review_scores_checkin')
ColumnReport('review_scores_communication')
ColumnReport('review_scores_location')
ColumnReport('review_scores_value')

len(data[['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']].loc[(data['review_scores_rating'].isna()) & (data['review_scores_accuracy'].isna()) & (data['review_scores_cleanliness'].isna()) & (data['review_scores_checkin'].isna()) & (data['review_scores_communication'].isna()) & (data['review_scores_location'].isna()) & (data['review_scores_value'].isna())])

#stwierdzono, że brakujące wartości w kolumnach dot. rankingów występują dla tych samych obserwacji - w przybliżeniu, pozostawiono póki co (do uzupełnienia w późniejszym etapie)

ColumnReport('instant_bookable')

# konwersja na typ bool

data['instant_bookable'] = data['instant_bookable'].map({'t':True, 'f':False})
data['instant_bookable'] = data['instant_bookable'].astype(bool)

ColumnReport('calculated_host_listings_count')
ColumnReport('calculated_host_listings_count_entire_homes')
ColumnReport('calculated_host_listings_count_private_rooms')
ColumnReport('calculated_host_listings_count_shared_rooms')

"""##Uzupełnienie lub usunięcie brakujących danych"""

#### kolejnym krokiem będzie manipulowanie danymi w celu uzupełnienia wartości NaN lub pozbycie się ich - w zależności od przypadku,
#jeszcze raz wyświetlono kolumny z procentowym udziałem wartości nullowych

empties = data.isna().sum() / len(data)
empties.sort_values(ascending=False).loc[empties > 0.0] * 100

data.loc[data['host_name'].isna()]

#po wejściu w link do ogłoszenia i próba odszukania informacji o hoście kończy się błędem strony, zatem braki danych wynikają najpewniej z jakiegoś niedopatrzenia po stronie AirBnB - obserwacja do usunięcia

data = data.drop(data.loc[data['host_name'].isna()].index)

len(data.loc[data['bathrooms'].isna()])

# jest tylko 5 obserwacji z brakiem liczby bathroom, uzupełnione zostaną wartością najczęściej występującą

print(data['bathrooms'].describe())
print(data['bathrooms'].median())

# najczęściej występującą daną jest 1.0, zatem tą wartością zostanie uzupełniona kolumna bathrooms

data['bathrooms'].fillna(data['bathrooms'].median(), inplace=True)

#sprawdzenie najczęściej występujących wartości w 'bathrooms_text dla bathroom == 1 i uzupełnienie braków tą wartością

data['bathrooms_text'].loc[data['bathrooms'] == 1.0].value_counts()
data['bathrooms_text'].fillna('1 bath', inplace=True)

#oceny rankingowe, ogląd danych
data[['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']].describe()

#zdecydowano się na uzupełnienie braków z każdej z w/w kolumn medianą wartości danej kolumny

for C in ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']:
  data[C].fillna(data[C].median(), inplace=True)

data['host_location'].value_counts()

#data['host_location'].isin(['Denmark']).sum()

print('host in Copenhagen: ',(data['host_location'].dropna().str.contains("Copenhagen").sum()) / len(data['host_location'].dropna()))
print('host in Denmark:    ',(data['host_location'].dropna().str.contains("Denmark").sum()) / len(data['host_location'].dropna()))

#analizując kilka najczęściej występujących wartości uznano, że 85% hostów pochodzi z Kopenhagi, natomiast ponad 98% z Danii, zdecydowano się uzupełnić wartością 'Denmark'

data['host_location'].fillna('Denmark', inplace=True)

data[['host_acceptance_rate']].describe()

#uzupełnienie braków wartością mediany
#data['host_acceptance_rate'].fillna(data['host_acceptance_rate'].median(), inplace=True)

data['host_acceptance_rate'].fillna(data['host_acceptance_rate'].median(), inplace=True)

### funkcja do usuwania outlierów zwracająca cały zbiór danych już z usuniętymi outlierami z podanej kolumny

def GetRidOfOutliers(whole_data, column_with_outliers):
  Q1 = whole_data[column_with_outliers].quantile(0.25)
  Q3 = whole_data[column_with_outliers].quantile(0.75)
  IOR = Q3 - Q1
  lower_lim = Q1 - 1.5 * IOR
  upper_lim = Q3 + 1.5 * IOR
  outliers_low = (whole_data[column_with_outliers] < lower_lim)
  outliers_up = (whole_data[column_with_outliers] > upper_lim)
  return whole_data[-(outliers_low | outliers_up)]

GetRidOfOutliers(data, 'price')['price'].describe()

### uzupełnienie Price ###

data['price'].describe()

#wykres pudełkowy z pozbyciem się outlierów z kolumny price

sns.boxplot(GetRidOfOutliers(data, 'price'), x='room_type', y='price')

#zdecydowano się nie uzupełniać wartości w kolumnie Price
#34% wartości to bardzo duży udział, można byłoby spróbować przeprowadzić głębszą analizę danych
#przy użyciu kombinacji metod uczenia maszynowego (regresji, lasu losowego, PCA) w celu próby przewidzenia wartości
#na podstawie innych danych, ale nie zdecydowano się tutaj na taki krok

"""# 4. Badanie zależności pomiędzy zmiennymi

##Spojrzenie na dane po ich edycji
"""

### ANALIZA DANYCH ###

data.info()

data.describe(include=np.number)

"""##Ogólna macierz korelacji Pearsona"""

#wyplotowano ogólną macierz pearsona ze wszystkich kolumn z danymi numerycznymi, aby zauważyć sektory o większej korelacji, którymi się można przyjrzeć bliżej

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
corr = data.select_dtypes(include=numerics).corr(method='pearson', min_periods=1, numeric_only=True)
plt.figure(figsize=(20, 20))
sns.set(font_scale=0.5)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

"""##Ogólna macierz korelacji Pearsona"""

corr = data.select_dtypes(include=numerics).corr(method='spearman', min_periods=1, numeric_only=True)
plt.figure(figsize=(20, 20))
sns.set(font_scale=0.5)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

"""##Wstępne obserwacje"""

'''
co zostało zauważone:

1) korelacja pomiędzy kolumnami dotyczącymi liczby ofert danego hosta - do zbadania
2) korelacja pomiędzy wartościami dotyczącymi cech lokalu: liczba miejsc zakwaterowania, liczba łóżek, łazienek, sypialni,
co ciekawe występuje niewielka zależność również pomiędzy ceną a liczbą wszystkich wynajmowanych przez hosta apartamentów i prywatnych pokoi w Kopenhadze - do zbadania
3) kolejny sektor korelacji zawiera kolumny dotyczące warunków rezerwacji (kolumny maximum_nights etc.) - skorelowane ze sobą w sposób naturalny, bez dalszego badania
4) kolejny sektor to kolumny dotyczące dostępności lokalu do wynajęcia, korelacja występuje również ze wskaźnikiem akceptacji rezerwacji przez hosta  - skorelowane ze sobą w sposób naturalny, bez dalszego badania
5) sektor danych związanych z recenzjami, co ciekawe występuje korelacja ujemna z niektórymi aspektami systemu oceny, do przyjrzenia się bliżej - do zbadania
6) coś, co nie wynika z macierzy korelacji, ale jest czymś naturalnym, to korelacja pomiędzy współrzędnymi a dzielnicą - do zbadania
'''

"""##Dane dotyczące liczby ofert"""

data_listning = data[['host_listings_count', 'host_total_listings_count', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms']]
sns.set_theme(style="white")
corr = data_listning.corr(method='spearman', min_periods=1, numeric_only=True)
plt.figure(figsize=(5, 5))
sns.set(font_scale=0.7)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

plt.figure(figsize=(6, 6))
sns.pairplot(data_listning)

"""##Dane dotyczące charakterystyki lokalu"""

data_character = data[['accommodates', 'bathrooms', 'bedrooms', 'beds', 'price']]
#data_character = GetRidOfOutliers(data_character, 'price')

corr = data[['accommodates', 'bathrooms', 'bedrooms', 'beds', 'price']].corr(method='spearman', min_periods=1, numeric_only=True)
plt.figure(figsize=(5, 5))
sns.set(font_scale=0.7)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

corr = data_character.corr(method='pearson', min_periods=1, numeric_only=True)
plt.figure(figsize=(5, 5))
sns.set(font_scale=0.7)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

sns.pairplot(data_character)

sns.relplot(x="price", y="accommodates", hue="room_type", size='bathrooms',
            alpha=0.35, palette="muted", sizes=(40,160),
            height=6, data=GetRidOfOutliers(data, 'price'))

#na wykresie widać, że prywatne pokoje charakteryzują się mniejszą ceną za wynajem niż całe apartamenty

"""##Dane dotyczące ocen lokalu"""

# 3) korelacja pomiędzy ilością, a jakością

data_review = data[[
'number_of_reviews',
'number_of_reviews_ltm',
'number_of_reviews_l30d',
'review_scores_rating',
'review_scores_accuracy',
'review_scores_cleanliness',
'review_scores_checkin',
'review_scores_communication',
'review_scores_location',
'review_scores_value',
'reviews_per_month'
]]

corr = data_review.corr(method='spearman', min_periods=1, numeric_only=True)
plt.figure(figsize=(10, 10))
sns.set(font_scale=0.7)
sns.heatmap(corr, annot=True, cmap='vlag', vmin=-1, vmax=1, linewidths=1)

sns.relplot(x='review_scores_communication', y='number_of_reviews', hue="room_type", size='price',
            alpha=0.35, palette="muted", sizes=(40,160),
            height=6, data=GetRidOfOutliers(GetRidOfOutliers(GetRidOfOutliers(data, 'review_scores_communication'), 'price'), 'number_of_reviews'))

#ciekawa zależność pomiędzy całkowitą liczbą recenzji a oceną za komunikację, widać na wykresie naturalne układające się linie trendu, których charakteru jednak nie udało się sprecyzować
#nie udało się odszukać zmiennej, która precyzowałaby którą z kilku linii trendów powinno się uwzględnić

GetRidOfOutliers(GetRidOfOutliers(GetRidOfOutliers(data, 'review_scores_communication'), 'price'), 'number_of_reviews').plot(kind='scatter', x='review_scores_communication', y='number_of_reviews')

sns.pairplot(data_review)

"""##Lokalizacja i dzielnice"""

#oferty danego hosta, z podziałem na dzielnice jak i na cenę (po usunięciu outlierów)

sns.set_theme(style="white")
sns.relplot(x="longitude", y="latitude", hue="neighbourhood_cleansed", size='price',
            alpha=0.35, palette="muted", sizes=(5,55),
            height=8, data=GetRidOfOutliers(data, 'price'))

#na wykresie widać ładnie zarysowane dzielnice
#można wykorzystać nakładając dane na prawdziwą mapę Kopenhagi

"""https://pl.copenhagenmap360.com/img/0/kopenhaga-dzielnice-mapa.jpg

# 5. Badanie zależności pomiędzy danymi kategorycznymi

##Funkcja do obliczania współczynnika Cramera
"""

#zdefiniowanie funkcji do obliczania współczynnika Cramera
def cramers_corrected_stat(x,y):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    result=-1
    if len(x.value_counts())==1 :
        print("First variable is constant")
    elif len(y.value_counts())==1:
        print("Second variable is constant")
    else:
        conf_matrix=pd.crosstab(x, y)

        if conf_matrix.shape[0]==2:
            correct=False
        else:
            correct=True

        chi2 = ss.chi2_contingency(conf_matrix, correction=correct)[0]

        n = sum(conf_matrix.sum())
        phi2 = chi2/n
        r,k = conf_matrix.shape
        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
        rcorr = r - ((r-1)**2)/(n-1)
        kcorr = k - ((k-1)**2)/(n-1)
        result=np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))
    return round(result,6)

#test funkcji
cramers_corrected_stat(data['neighbourhood_cleansed'], data['price'])

"""##Odszukanie zmiennych kategorycznych"""

#zmienne kategoryczne ->
'''
host_is_superhost
host_has_profile_pic
host_identity_verified
neighbourhood_cleansed
property_type
room_type
instant_bookable
'''

plt.figure(figsize=(6, 6))
sns.boxplot(GetRidOfOutliers(data, 'price'), x='host_is_superhost', y='price')

"""##Obliczanie współczynnika Cramera"""

categoricals = [
'host_is_superhost',
'host_has_profile_pic',
'host_identity_verified',
'neighbourhood_cleansed',
'property_type',
'room_type',
'instant_bookable'
]

#for cat1 in categoricals:
#  for cat2 in categoricals:
#    print(cat1, cat2, cramers_corrected_stat(data[cat1], data[cat2]))

for cat1 in categoricals:
  for cat2 in categoricals:
    if cramers_corrected_stat(data[cat1], data[cat2]) > 0.1 and cramers_corrected_stat(data[cat1], data[cat2]) != 1.0:
      print(cat1, cat2, cramers_corrected_stat(data[cat1], data[cat2]))

'''
pary zmiennych kategorycznych o wysokim wspołczynniku Cramera V:
  host_is_superhost property_type 0.179954
  neighbourhood_cleansed property_type 0.153055
  property_type instant_bookable 0.189963

oraz, co oczywiste, zmienne praktycznie identyczne:
  property_type room_type 0.99857

'''

"""##Wizualizacja na wykresie mozaikowym na przykładzie jednej pary zmiennych kategorycznych"""

mosaic(data, ['host_is_superhost', 'room_type'])

"""# 6. Badanie zależności pomiędzy zmienną kategoryczną a numeryczną

##Przygotowanie podzbioru danych
"""

### regresja liniowa - zbadanie zaleśności ceny od dzielnicy oraz typu pokoju

data_short = data[['room_type', 'neighbourhood_cleansed', 'price']]
data_short.head()

# zmiana wartości dzielnicy jako dane kategoryczne na oddzielne kolumny 1/0

columns_encode = ['room_type', 'neighbourhood_cleansed']
data_short = pd.get_dummies(data_short, columns=columns_encode, prefix=columns_encode)

data_short = data_short.dropna()

data_short = GetRidOfOutliers(data_short, 'price')

data_short['price'].plot.hist()

data_short.rename(columns=
{
       'room_type_Entire home/apt' : 'room_type_entire_home_apt',
       'room_type_Hotel room' : 'room_type_hotel_room',
       'room_type_Private room' : 'room_type_private_room',
       'room_type_Shared room' : 'room_type_shared_room',
       'neighbourhood_cleansed_Amager Vest' : 'n_Amager_Vest',
       'neighbourhood_cleansed_Amager st' : 'n_Amager_st',
       'neighbourhood_cleansed_Bispebjerg' : 'n_Bispebjerg',
       'neighbourhood_cleansed_Brnshj-Husum' : 'n_Brnshj_Husum',
       'neighbourhood_cleansed_Frederiksberg' : 'n_Frederiksberg',
       'neighbourhood_cleansed_Indre By' : 'n_Indre_By',
       'neighbourhood_cleansed_Nrrebro' : 'n_Nrrebro',
       'neighbourhood_cleansed_Valby' : 'n_Valby',
       'neighbourhood_cleansed_Vanlse' : 'n_Vanlse',
       'neighbourhood_cleansed_Vesterbro-Kongens Enghave' : 'n_Vesterbro_Kongens_Enghave',
       'neighbourhood_cleansed_sterbro' : 'n_sterbro'
}, inplace=True)

data_short.head()

"""##Zbadanie zależności ceny od dzielnicy"""

model = smf.ols(
    '''price ~
n_Amager_Vest +
n_Amager_st +
n_Bispebjerg +
n_Brnshj_Husum +
n_Frederiksberg +
n_Indre_By +
n_Nrrebro +
n_Valby +
n_Vanlse +
n_Vesterbro_Kongens_Enghave +
n_sterbro''',
data=data_short).fit()


'''
price ~
room_type_entire_home_apt +
room_type_hotel_room +
room_type_shared_room +
room_type_private_room +
n_Amager_Vest +
n_Amager_st +
n_Bispebjerg +
n_Brnshj_Husum +
n_Frederiksberg +
n_Indre_By +
n_Nrrebro +
n_Valby +
n_Vanlse +
n_Vesterbro_Kongens_Enghave +
n_sterbro'''

print(model.summary())

model.predict(model.model.data.frame)
data_model_preds = pd.concat([data_short, model.predict(data_short).rename('predict')], axis =1)
data_model_preds
print(np.sqrt(model.mse_resid))

"""##Zbadanie zależności ceny od typu lokalu"""

model = smf.ols(
'''
price ~
room_type_entire_home_apt +
room_type_hotel_room +
room_type_shared_room +
room_type_private_room
''',
data=data_short).fit()

print(model.summary())

model.predict(model.model.data.frame)
data_model_preds = pd.concat([data_short, model.predict(data_short).rename('predict')], axis =1)
data_model_preds
print(np.sqrt(model.mse_resid))

"""##Stworzenie modelu regresji liniowej badającej zależność ceny od obu danych kategorycznych"""

model = smf.ols(
'''
price ~
room_type_entire_home_apt +
room_type_hotel_room +
room_type_private_room +
room_type_shared_room +
n_Amager_Vest +
n_Amager_st +
n_Bispebjerg +
n_Brnshj_Husum +
n_Frederiksberg +
n_Indre_By +
n_Nrrebro +
n_Valby +
n_Vanlse +
n_Vesterbro_Kongens_Enghave +
n_sterbro''',
data=data_short).fit()

print(model.summary())

model.predict(model.model.data.frame)
data_model_preds = pd.concat([data_short, model.predict(data_short).rename('predict')], axis =1)
data_model_preds
print(np.sqrt(model.mse_resid))

"""##Ocena modelu"""

#Błąd w każdym przypadku regresji liniowej jest duży, model nie posiada wystarczająco dużo danych potrzebnych do lepszej predykcji ceny
#Lokalizacja (dzielnica), podobnie jak typ lokalu ma wpływ na cenę, jest ona jednak uzależniona od wielu innych aspektów, prawdopodobnie od cech stricte lokalu (miejsc zakwaterowania etc)
#Do modelu trzeba byłoby metodą prób i błędów pobierać kolejne zmienne w celu zminimalizowania błędu
#Dobrym pomysłem byłoby również stworzenie kilku modeli, podzielonych na bazie zmiennych kategorycznych np. oddzielnie dla prywatnych pokoi i oddzielnie dla aprartamentów
#Dane dotyczące podziału na rodzaj lokalu charakteryzują się wysokim współczynnikiem P, który jest powyżej założonego progu istotności p=0,05

#Analizując model złożony z dnaych o dzielnicy i typu lokalu:
#Drogo wypada cena za apartament w Indre By - 77.57 (intercept) + 87.37 (apartament) + 52.41 (Indre By) = 217,35$
#Natomast tanio wypada pokój współdzielony w dzielnicy Bispebjerg - 36,45$ - lecz jak wspomniano wyżej, pokój współdzielony charakteruzuje się wysiokim P, więc nie jest to wniosek podparty mocnym argumentem (nie można odrzucić hipotezy zerowej)

plt.figure(figsize=(6, 6))
sns.boxplot(GetRidOfOutliers(data, 'price'), x='price', y='neighbourhood_cleansed')

plt.figure(figsize=(6, 6))
sns.boxplot(GetRidOfOutliers(data, 'price'), x='price', y='room_type')

"""# 7. Podpis"""

#Wykonał:
#Wojciech Cendrowski pd4255
#22.01.2024

"""#8. XGBoost

"""

#XGBoost
from xgboost import XGBClassifier
from xgboost import XGBRegressor
import xgboost as xgb

data.info()

data.columns

data_price = data.dropna(subset = ['price'])
data_price = data_price[['host_since',
       'host_acceptance_rate', 'host_is_superhost', 'host_total_listings_count',
       'host_has_profile_pic', 'host_identity_verified',
       'neighbourhood_cleansed',
       'room_type', 'accommodates', 'bathrooms', 'bedrooms',
       'beds', 'price', 'minimum_nights', 'maximum_nights',
       'minimum_minimum_nights', 'maximum_minimum_nights',
       'minimum_maximum_nights', 'maximum_maximum_nights',
       'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30',
       'availability_60', 'availability_90', 'availability_365',
       'number_of_reviews', 'number_of_reviews_ltm',
       'number_of_reviews_l30d',
       'review_scores_rating', 'review_scores_accuracy',
       'review_scores_cleanliness', 'review_scores_checkin',
       'review_scores_communication', 'review_scores_location',
       'review_scores_value', 'instant_bookable',
       'calculated_host_listings_count',
      'reviews_per_month']]

data_price.info()

data_price = pd.get_dummies(data_price, columns = ['neighbourhood_cleansed'])

data_price.info()









data_price_apart = data_price.loc[data_price['room_type'] == 'Entire home/apt']
data_price_rooms = data_price.loc[data_price['room_type'] == 'Private room']

data_price_apart.drop(columns=['host_since'], inplace=True)
data_price_rooms.drop(columns=['host_since'], inplace=True)

data_price_apart.drop(columns=['room_type'], inplace=True)
data_price_rooms.drop(columns=['room_type'], inplace=True)





from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data_price_apart.drop(columns=['price']), data_price_apart['price'], test_size=.3)
model = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)
# fit model
model.fit(X_train, y_train)
# make predictions
preds = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print(rmse)

from sklearn.metrics import mean_squared_error

xgb.plot_importance(model)
plt.high_layout()



